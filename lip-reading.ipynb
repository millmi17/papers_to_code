{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# This notebook is going over and trying to recreate the model used the paper LIPNET: END-TO-END SENTENCE-LEVEL LIPREADING. this notebook was also inspired by Nicholas Renotte","metadata":{}},{"cell_type":"markdown","source":"## in this we are only using on speaker in the grid data set. This will work for more speakers, but the time to train the model would take too long. This already took 6 min an epoch using a gpu on kaggle for only data for one speaker","metadata":{"execution":{"iopub.status.busy":"2023-02-18T02:40:14.336565Z","iopub.execute_input":"2023-02-18T02:40:14.336977Z","iopub.status.idle":"2023-02-18T02:40:14.342126Z","shell.execute_reply.started":"2023-02-18T02:40:14.336941Z","shell.execute_reply":"2023-02-18T02:40:14.341146Z"}}},{"cell_type":"code","source":"# importing required libraries\nimport numpy as np \nimport pandas as pd \nimport os\nfrom tensorflow.nn import ctc_loss\nimport tensorflow as tf\nfrom keras.models import load_model\nimport cv2\nimport keras\nfrom tensorflow.keras.models import Sequential \nfrom tensorflow.keras.layers import Input,Conv3D, Dense, Dropout, Bidirectional, MaxPool3D, Activation, Reshape, BatchNormalization,GRU, TimeDistributed, Flatten\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n","metadata":{"execution":{"iopub.status.busy":"2023-02-18T02:50:11.001323Z","iopub.execute_input":"2023-02-18T02:50:11.001798Z","iopub.status.idle":"2023-02-18T02:50:16.181635Z","shell.execute_reply.started":"2023-02-18T02:50:11.001753Z","shell.execute_reply":"2023-02-18T02:50:16.180378Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# this function takes a video and loads the images. It also crops around the lips\n# in the paper they use a lip detector to extract the lips. That is something\n# that can be implemented in another iteration of this\ndef load_frames(path):\n    outputframes = []\n    video = cv2.VideoCapture(path)\n    countframes = video.get(cv2.CAP_PROP_FRAME_COUNT)\n    for i in range(int(countframes)):\n        video.set(cv2.CAP_PROP_POS_FRAMES, 1)\n        ret, frame = video.read()\n        frame = tf.image.rgb_to_grayscale(frame)\n\n        outputframes.append(tf.image.per_image_standardization(frame[190:236,80:220,:]))\n    return outputframes","metadata":{"execution":{"iopub.status.busy":"2023-02-18T02:50:16.183917Z","iopub.execute_input":"2023-02-18T02:50:16.184592Z","iopub.status.idle":"2023-02-18T02:50:16.195234Z","shell.execute_reply.started":"2023-02-18T02:50:16.184554Z","shell.execute_reply":"2023-02-18T02:50:16.192265Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# creating the vocabulary\nvocab = [x for x in \"abcdefghijklmnopqrstuvwxyz'?!123456789 \"]","metadata":{"execution":{"iopub.status.busy":"2023-02-18T02:50:16.196752Z","iopub.execute_input":"2023-02-18T02:50:16.197241Z","iopub.status.idle":"2023-02-18T02:50:16.210156Z","shell.execute_reply.started":"2023-02-18T02:50:16.197182Z","shell.execute_reply":"2023-02-18T02:50:16.209189Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# these are creating a way to transform a charcter to a number and number to letter\ncharacternum = tf.keras.layers.StringLookup(vocabulary = vocab,oov_token=\"\")\nnumcharacter = tf.keras.layers.StringLookup(vocabulary = vocab,oov_token=\"\",invert = True)","metadata":{"execution":{"iopub.status.busy":"2023-02-18T02:50:16.212785Z","iopub.execute_input":"2023-02-18T02:50:16.213134Z","iopub.status.idle":"2023-02-18T02:50:18.974982Z","shell.execute_reply.started":"2023-02-18T02:50:16.213101Z","shell.execute_reply":"2023-02-18T02:50:18.974052Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"2023-02-18 02:50:16.304196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-02-18 02:50:16.400626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-02-18 02:50:16.401414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-02-18 02:50:16.403000: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-02-18 02:50:16.403324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-02-18 02:50:16.403998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-02-18 02:50:16.404701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-02-18 02:50:18.590957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-02-18 02:50:18.591811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-02-18 02:50:18.592464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-02-18 02:50:18.593038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15401 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"}]},{"cell_type":"code","source":"# this is loading the aligned labels for what the speaker is saying\n# we get rid of the parts for silence\ndef load_align(path):\n    #print(path)\n    with open(path, 'r') as f: \n        lines = f.readlines() \n    sentence = []\n    \n    for line in lines:\n        L =line.split()[2]\n        if L == 'sil':\n            continue\n        else:\n            if len(sentence)>1:\n                sentence.append(' ')\n            for j in L:\n                sentence.append(j)\n    return characternum(sentence)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-18T02:50:18.976630Z","iopub.execute_input":"2023-02-18T02:50:18.977067Z","iopub.status.idle":"2023-02-18T02:50:18.983832Z","shell.execute_reply.started":"2023-02-18T02:50:18.977028Z","shell.execute_reply":"2023-02-18T02:50:18.982549Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# takes in video file name and ruetruns the frames and the alignments of words\ndef load(path):\n    \n    finalpath =bytes.decode(path.numpy())\n   \n    filename = finalpath.split('/')[-1]\n    filename = filename.split('.')[0]\n    \n    framepath = os.path.join('/kaggle','input','grid-1-speaker','s4.mpg_vcd','s4',finalpath)\n    alignpath = os.path.join('/kaggle','input','grid-1-speaker','s4','align',filename+'.align')\n    frames = load_frames(framepath)\n    align = load_align(alignpath)\n    return frames, align","metadata":{"execution":{"iopub.status.busy":"2023-02-18T02:50:18.985566Z","iopub.execute_input":"2023-02-18T02:50:18.986390Z","iopub.status.idle":"2023-02-18T02:50:18.997107Z","shell.execute_reply.started":"2023-02-18T02:50:18.986299Z","shell.execute_reply":"2023-02-18T02:50:18.996257Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# this is needed to create the load function to work with tensorflow\ndef final_function(path:str):\n    return tf.py_function(func=load, inp=[path], Tout=(tf.float32,tf.int64))","metadata":{"execution":{"iopub.status.busy":"2023-02-18T02:50:18.998622Z","iopub.execute_input":"2023-02-18T02:50:18.999021Z","iopub.status.idle":"2023-02-18T02:50:19.008271Z","shell.execute_reply.started":"2023-02-18T02:50:18.998988Z","shell.execute_reply":"2023-02-18T02:50:19.007329Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# this takes the data and shuffles, then runs our load function\n# then makes a batch of size 2 and prefetchs some data for faster training.\n# saving 450 videos for training and the rest for testing\ndata = tf.data.Dataset.list_files('/kaggle/input/grid-1-speaker/s4.mpg_vcd/s4/*.mpg', shuffle=None, seed=42)\ndata = data.shuffle(500, reshuffle_each_iteration=False)\ndata = data.map(final_function)\ndata = data.padded_batch(2, padded_shapes=([75,None,None,None],[40]))\ndata = data.prefetch(tf.data.AUTOTUNE)\n# Added for split \ntrain = data.take(450)\ntest = data.skip(450)","metadata":{"execution":{"iopub.status.busy":"2023-02-18T02:50:19.009866Z","iopub.execute_input":"2023-02-18T02:50:19.010236Z","iopub.status.idle":"2023-02-18T02:50:19.659928Z","shell.execute_reply.started":"2023-02-18T02:50:19.010188Z","shell.execute_reply":"2023-02-18T02:50:19.658960Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"frames, alignments = data.as_numpy_iterator().next()\nframes.shape","metadata":{"execution":{"iopub.status.busy":"2023-02-18T02:50:19.661237Z","iopub.execute_input":"2023-02-18T02:50:19.661580Z","iopub.status.idle":"2023-02-18T02:50:21.698130Z","shell.execute_reply.started":"2023-02-18T02:50:19.661546Z","shell.execute_reply":"2023-02-18T02:50:21.697225Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"2023-02-18 02:50:19.689577: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(2, 75, 46, 140, 1)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# this is for the ctc loss. This code was copied from an example on the tensorflow website.\n# this loss is needed since our model will predict characters from our vocab for \n# each of the 75 frames. but the real alignments don't have 75 characters\n# thus this loss deals with this difference\ndef CTCLoss(y_true, y_pred):\n    batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n    input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n    label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n\n    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n\n    loss = tf.keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n    return loss\n","metadata":{"execution":{"iopub.status.busy":"2023-02-18T02:50:21.701236Z","iopub.execute_input":"2023-02-18T02:50:21.701567Z","iopub.status.idle":"2023-02-18T02:50:21.708339Z","shell.execute_reply.started":"2023-02-18T02:50:21.701540Z","shell.execute_reply":"2023-02-18T02:50:21.707399Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# this part is here to load a partialy trained model. Since it took a long time\n# i needed to train over multiple sessions. \nmodel = load_model('/kaggle/input/final-lip-read/saved-model-v3-45.hdf5',compile=False)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-02-18T02:50:21.709957Z","iopub.execute_input":"2023-02-18T02:50:21.710722Z","iopub.status.idle":"2023-02-18T02:50:22.850036Z","shell.execute_reply.started":"2023-02-18T02:50:21.710687Z","shell.execute_reply":"2023-02-18T02:50:22.849003Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Model: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv3d_9 (Conv3D)            (None, 75, 46, 140, 32)   832       \n_________________________________________________________________\nactivation_9 (Activation)    (None, 75, 46, 140, 32)   0         \n_________________________________________________________________\nmax_pooling3d_9 (MaxPooling3 (None, 75, 23, 70, 32)    0         \n_________________________________________________________________\nconv3d_10 (Conv3D)           (None, 75, 23, 70, 64)    51264     \n_________________________________________________________________\nactivation_10 (Activation)   (None, 75, 23, 70, 64)    0         \n_________________________________________________________________\nmax_pooling3d_10 (MaxPooling (None, 75, 11, 35, 64)    0         \n_________________________________________________________________\nconv3d_11 (Conv3D)           (None, 75, 11, 35, 96)    55392     \n_________________________________________________________________\nactivation_11 (Activation)   (None, 75, 11, 35, 96)    0         \n_________________________________________________________________\nmax_pooling3d_11 (MaxPooling (None, 75, 5, 17, 96)     0         \n_________________________________________________________________\ntime_distributed_3 (TimeDist (None, 75, 8160)          0         \n_________________________________________________________________\ngru_6 (GRU)                  (None, 75, 256)           6465024   \n_________________________________________________________________\ndropout_6 (Dropout)          (None, 75, 256)           0         \n_________________________________________________________________\ngru_7 (GRU)                  (None, 75, 256)           394752    \n_________________________________________________________________\ndropout_7 (Dropout)          (None, 75, 256)           0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 75, 41)            10537     \n=================================================================\nTotal params: 6,977,801\nTrainable params: 6,977,801\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"# this is the code for creating the model like the one from the paper\n'''model = Sequential()\nmodel.add(Input(shape=(75,46,140,1)))\nmodel.add(Conv3D(32,kernel_size = (5,5,1),padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPool3D((1,2,2)))\n\nmodel.add(Conv3D(64,kernel_size = (5,5,1),padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPool3D((1,2,2)))\n\nmodel.add(Conv3D(96,kernel_size = (3,3,1),padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPool3D((1,2,2)))\n\n\nmodel.add(TimeDistributed(Flatten()))\nmodel.add(GRU(256,recurrent_initializer='orthogonal',return_sequences=True))\nmodel.add(Dropout(0.5))\n\nmodel.add(GRU(256,recurrent_initializer='orthogonal',return_sequences=True))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(characternum.vocabulary_size()+1, kernel_initializer='he_normal', activation='softmax'))\n\nmodel.summary()\n'''","metadata":{"execution":{"iopub.status.busy":"2023-02-18T02:50:22.851538Z","iopub.execute_input":"2023-02-18T02:50:22.851869Z","iopub.status.idle":"2023-02-18T02:50:22.859167Z","shell.execute_reply.started":"2023-02-18T02:50:22.851835Z","shell.execute_reply":"2023-02-18T02:50:22.858071Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"\"model = Sequential()\\nmodel.add(Input(shape=(75,46,140,1)))\\nmodel.add(Conv3D(32,kernel_size = (5,5,1),padding='same'))\\nmodel.add(Activation('relu'))\\nmodel.add(MaxPool3D((1,2,2)))\\n\\nmodel.add(Conv3D(64,kernel_size = (5,5,1),padding='same'))\\nmodel.add(Activation('relu'))\\nmodel.add(MaxPool3D((1,2,2)))\\n\\nmodel.add(Conv3D(96,kernel_size = (3,3,1),padding='same'))\\nmodel.add(Activation('relu'))\\nmodel.add(MaxPool3D((1,2,2)))\\n\\n\\nmodel.add(TimeDistributed(Flatten()))\\nmodel.add(GRU(256,recurrent_initializer='orthogonal',return_sequences=True))\\nmodel.add(Dropout(0.5))\\n\\nmodel.add(GRU(256,recurrent_initializer='orthogonal',return_sequences=True))\\nmodel.add(Dropout(0.5))\\n\\nmodel.add(Dense(characternum.vocabulary_size()+1, kernel_initializer='he_normal', activation='softmax'))\\n\\nmodel.summary()\\n\""},"metadata":{}}]},{"cell_type":"code","source":"fr ,r =data.as_numpy_iterator().next()\nprint(fr.shape)\ny = model.predict(fr)\ny.shape","metadata":{"execution":{"iopub.status.busy":"2023-02-18T02:50:22.860816Z","iopub.execute_input":"2023-02-18T02:50:22.861149Z","iopub.status.idle":"2023-02-18T02:50:29.753556Z","shell.execute_reply.started":"2023-02-18T02:50:22.861116Z","shell.execute_reply":"2023-02-18T02:50:29.752628Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"(2, 75, 46, 140, 1)\n","output_type":"stream"},{"name":"stderr","text":"2023-02-18 02:50:25.124814: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"(2, 75, 41)"},"metadata":{}}]},{"cell_type":"code","source":"model.compile(optimizer=Adam(learning_rate=0.0001), loss=CTCLoss)","metadata":{"execution":{"iopub.status.busy":"2023-02-14T15:30:22.082680Z","iopub.execute_input":"2023-02-14T15:30:22.084789Z","iopub.status.idle":"2023-02-14T15:30:22.101895Z","shell.execute_reply.started":"2023-02-14T15:30:22.084751Z","shell.execute_reply":"2023-02-14T15:30:22.100471Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"filepath = \"saved-model-v3-{epoch:02d}.hdf5\"\ncheckpoint_callback = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=False, mode='min')\n\n#checkpoint_callback = ModelCheckpoint(filepath ='my_best_model.hdf5', monitor='loss',save_best_only=False, mode=’min’) ","metadata":{"execution":{"iopub.status.busy":"2023-02-14T15:30:22.105814Z","iopub.execute_input":"2023-02-14T15:30:22.108003Z","iopub.status.idle":"2023-02-14T15:30:22.114732Z","shell.execute_reply.started":"2023-02-14T15:30:22.107965Z","shell.execute_reply":"2023-02-14T15:30:22.113832Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-02-14T15:30:22.119386Z","iopub.execute_input":"2023-02-14T15:30:22.121173Z","iopub.status.idle":"2023-02-14T15:30:22.131892Z","shell.execute_reply.started":"2023-02-14T15:30:22.121138Z","shell.execute_reply":"2023-02-14T15:30:22.131006Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"\n# this code decodes the predictions for the model. it takes the batch input\n# it decodes the ctc results and then using a number to character function\n# takes the number and decodes it to its character and combines all of the \n# results for the same output\ndef decode_batch_predictions(pred):\n    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n    \n    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0]\n    # Iterate over the results and get back the text\n    output_text = []\n    for result in results:\n        result = tf.strings.reduce_join(numcharacter(result)).numpy().decode(\"utf-8\")\n        output_text.append(result)\n    return output_text","metadata":{"execution":{"iopub.status.busy":"2023-02-18T02:54:21.180580Z","iopub.execute_input":"2023-02-18T02:54:21.181466Z","iopub.status.idle":"2023-02-18T02:54:21.188395Z","shell.execute_reply.started":"2023-02-18T02:54:21.181428Z","shell.execute_reply":"2023-02-18T02:54:21.187286Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"fr ,r =data.as_numpy_iterator().next()","metadata":{"execution":{"iopub.status.busy":"2023-02-18T02:50:41.195045Z","iopub.execute_input":"2023-02-18T02:50:41.195418Z","iopub.status.idle":"2023-02-18T02:50:42.577763Z","shell.execute_reply.started":"2023-02-18T02:50:41.195388Z","shell.execute_reply":"2023-02-18T02:50:42.576670Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"a = fr\noutput = model.predict(a)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-18T02:51:00.040312Z","iopub.execute_input":"2023-02-18T02:51:00.040671Z","iopub.status.idle":"2023-02-18T02:51:00.115769Z","shell.execute_reply.started":"2023-02-18T02:51:00.040640Z","shell.execute_reply":"2023-02-18T02:51:00.114910Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"array([[[3.92715505e-04, 7.06533086e-04, 1.54368937e-01, ...,\n         2.32615207e-07, 1.55888556e-04, 2.01194052e-04],\n        [1.15997891e-05, 5.15149091e-04, 1.32713700e-04, ...,\n         6.10261119e-10, 1.64611185e-06, 3.40751831e-05],\n        [1.51826850e-06, 9.92895305e-01, 1.06430662e-05, ...,\n         1.36125311e-09, 1.27989244e-06, 1.12719827e-04],\n        ...,\n        [9.98312473e-01, 2.01988737e-08, 8.01099560e-08, ...,\n         3.29071874e-11, 1.67962195e-08, 1.68275111e-03],\n        [3.36059695e-03, 6.39720099e-08, 1.63123937e-09, ...,\n         1.48678999e-14, 5.75921311e-10, 9.96638298e-01],\n        [9.98790205e-01, 1.34166243e-08, 1.05079216e-07, ...,\n         2.63463071e-11, 2.43913156e-08, 1.20611303e-03]],\n\n       [[1.39042968e-04, 1.45364625e-04, 5.68868266e-03, ...,\n         1.09336282e-08, 1.35872624e-05, 1.79015144e-04],\n        [2.82467790e-05, 5.76017017e-04, 2.40705976e-05, ...,\n         5.82730086e-10, 1.19791548e-06, 3.44191387e-04],\n        [3.62313108e-06, 9.85427797e-01, 4.48850005e-06, ...,\n         1.99023020e-09, 3.94473363e-06, 1.08293304e-03],\n        ...,\n        [9.97384846e-01, 6.01500760e-09, 2.46182665e-08, ...,\n         6.30592800e-11, 5.21785193e-09, 2.61151395e-03],\n        [3.41471983e-03, 3.36580328e-08, 6.12290774e-10, ...,\n         4.63550605e-14, 1.76919548e-10, 9.96584058e-01],\n        [9.98631775e-01, 5.49922152e-09, 4.70861927e-08, ...,\n         3.04551731e-11, 9.65684865e-09, 1.36540981e-03]]], dtype=float32)"},"metadata":{}}]},{"cell_type":"markdown","source":"## below takes two examples and runs them through the model to see results and decodes the results.\n\n## we can see that it does a pretty good job at getting the correct words. It does have some errors. But overall does a good job","metadata":{}},{"cell_type":"code","source":"print('real output:',tf.strings.reduce_join(numcharacter(r[0])).numpy().decode(\"utf-8\"))\nprint('predicted output',decode_batch_predictions(output)[0])\n\nprint('second video')\nprint('real output:',tf.strings.reduce_join(numcharacter(r[1])).numpy().decode(\"utf-8\"))\nprint('predicted output',decode_batch_predictions(output)[1])","metadata":{"execution":{"iopub.status.busy":"2023-02-18T02:57:21.034334Z","iopub.execute_input":"2023-02-18T02:57:21.034694Z","iopub.status.idle":"2023-02-18T02:57:21.055406Z","shell.execute_reply.started":"2023-02-18T02:57:21.034663Z","shell.execute_reply":"2023-02-18T02:57:21.054479Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"real output: lay red in q six again\npredicted output lay red in x six again\nsecond video\nreal output: lay red with z zero soon\npredicted output lay red with z zero soon\n","output_type":"stream"}]}]}