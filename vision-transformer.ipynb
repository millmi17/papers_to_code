{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-06T13:25:44.014169Z","iopub.execute_input":"2023-04-06T13:25:44.014622Z","iopub.status.idle":"2023-04-06T13:25:44.020535Z","shell.execute_reply.started":"2023-04-06T13:25:44.014573Z","shell.execute_reply":"2023-04-06T13:25:44.019229Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras import layers\nfrom tensorflow import keras\nimport tensorflow_addons as tfa","metadata":{"execution":{"iopub.status.busy":"2023-04-06T13:25:44.022927Z","iopub.execute_input":"2023-04-06T13:25:44.023378Z","iopub.status.idle":"2023-04-06T13:25:44.031251Z","shell.execute_reply.started":"2023-04-06T13:25:44.023281Z","shell.execute_reply":"2023-04-06T13:25:44.029898Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"batch_size = 128\nimg_height = 256\nimg_width = 256\ntrain_datagen = ImageDataGenerator(rescale=1./255,\n        \n        \n        horizontal_flip=True,\n        rotation_range=0.1,\n        zoom_range=0.2,\n    \n    validation_split=0.2) # set validation split\n\ntrain_generator = train_datagen.flow_from_directory(\n    '/kaggle/input/intel-image-classification/seg_train/seg_train',\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    shuffle=True,\n    seed=42,\n    \n    subset='training') # set as training data\n\nvalidation_generator = train_datagen.flow_from_directory(\n    '/kaggle/input/intel-image-classification/seg_train/seg_train', # same directory as training data\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    shuffle=True,\n    seed=42,\n    \n    subset='validation') # set as validation data\n","metadata":{"execution":{"iopub.status.busy":"2023-04-06T13:25:44.035233Z","iopub.execute_input":"2023-04-06T13:25:44.035588Z","iopub.status.idle":"2023-04-06T13:25:46.896127Z","shell.execute_reply.started":"2023-04-06T13:25:44.035562Z","shell.execute_reply":"2023-04-06T13:25:46.895114Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Found 11230 images belonging to 6 classes.\nFound 2804 images belonging to 6 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mlp(x, size, dropoutrate):\n    \n    x = layers.Dense(size, activation=tf.nn.gelu)(x)\n    x = layers.Dropout(dropoutrate)(x)\n    return x","metadata":{"execution":{"iopub.status.busy":"2023-04-06T13:25:46.899031Z","iopub.execute_input":"2023-04-06T13:25:46.900045Z","iopub.status.idle":"2023-04-06T13:25:46.905574Z","shell.execute_reply.started":"2023-04-06T13:25:46.900003Z","shell.execute_reply":"2023-04-06T13:25:46.904526Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Patches(layers.Layer):\n    def __init__(self, patch_size):\n        super().__init__()\n        self.patch_size = patch_size\n\n    def call(self, images):\n        batch_size = tf.shape(images)[0]\n        # return batch size of batchsize, row of patches, columns of patchs, size of patches**2 *3\n        patches = tf.image.extract_patches(\n            images=images,\n            sizes=[1, self.patch_size, self.patch_size, 1],\n            strides=[1, self.patch_size, self.patch_size, 1],\n            rates=[1, 1, 1, 1],\n            padding=\"VALID\",\n        )\n        \n        patch_dims = patches.shape[-1]\n        \n        # this will reshape the patches instead of being x *x will now be X**2 in size so batchsize, x**2, size of patches**2 *3\n        patches = tf.reshape(patches, [batch_size, 256, patch_dims])\n       \n        return patches","metadata":{"execution":{"iopub.status.busy":"2023-04-06T13:25:46.907161Z","iopub.execute_input":"2023-04-06T13:25:46.907753Z","iopub.status.idle":"2023-04-06T13:25:46.918253Z","shell.execute_reply.started":"2023-04-06T13:25:46.907714Z","shell.execute_reply":"2023-04-06T13:25:46.917184Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import cv2\nimg = cv2.imread('/kaggle/input/intel-image-classification/seg_train/seg_train/buildings/10006.jpg')\nimg.shape\nimg = img[np.newaxis,:,:,:]\nimg.shape","metadata":{"execution":{"iopub.status.busy":"2023-04-06T13:25:46.921453Z","iopub.execute_input":"2023-04-06T13:25:46.922494Z","iopub.status.idle":"2023-04-06T13:25:46.933585Z","shell.execute_reply.started":"2023-04-06T13:25:46.922455Z","shell.execute_reply":"2023-04-06T13:25:46.932456Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"(1, 150, 150, 3)"},"metadata":{}}]},{"cell_type":"code","source":"class PatchEncoder(layers.Layer):\n    def __init__(self, num_patches, projection_dim):\n        super().__init__()\n        self.num_patches = num_patches\n        self.projection = layers.Dense(units=projection_dim)\n        self.position_embedding = layers.Embedding(\n            input_dim=num_patches, output_dim=projection_dim\n        )\n\n    def call(self, patch):\n        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n        encoded = self.projection(patch) + self.position_embedding(positions)\n        return encoded","metadata":{"execution":{"iopub.status.busy":"2023-04-06T13:25:46.935282Z","iopub.execute_input":"2023-04-06T13:25:46.935665Z","iopub.status.idle":"2023-04-06T13:25:46.943181Z","shell.execute_reply.started":"2023-04-06T13:25:46.935629Z","shell.execute_reply":"2023-04-06T13:25:46.941932Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def create_vit():\n    inputs = layers.Input(shape=(256,256,3))\n    # Augment data.\n    \n    # Create patches.\n    patches = Patches(16)(inputs)\n    # Encode patches.\n    encoded_patches = PatchEncoder(256, 64)(patches)\n    #transformer layers below\n    for _ in range(8):\n        # Layer normalization 1.\n        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n        # Create a multi-head attention layer.\n        attention_output = layers.MultiHeadAttention(\n            num_heads=6, key_dim=64, dropout=0.1\n        )(x1, x1)\n        # Skip connection 1.\n        x2 = layers.Add()([attention_output, encoded_patches])\n        # Layer normalization 2.\n        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n        # MLP.\n        x3 = mlp(x3, size=128, dropoutrate=0.1)\n        x4 = mlp(x3, size=64, dropoutrate=0.1)\n        # Skip connection 2.\n        encoded_patches = layers.Add()([x4, x2])\n        \n    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n    representation = layers.Flatten()(representation)\n    representation = layers.Dropout(0.5)(representation)\n    # Add MLP.\n    \n    features = mlp(representation, size=2048, dropoutrate=0.5)\n    features = mlp(features, size=1024, dropoutrate=0.5)\n    # Classify outputs.\n    logits = layers.Dense(6)(features)\n    # Create the Keras model.\n    model = keras.Model(inputs=inputs, outputs=logits)\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-04-06T13:25:46.944925Z","iopub.execute_input":"2023-04-06T13:25:46.945324Z","iopub.status.idle":"2023-04-06T13:25:46.958053Z","shell.execute_reply.started":"2023-04-06T13:25:46.945237Z","shell.execute_reply":"2023-04-06T13:25:46.956974Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"model = create_vit()\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-04-06T13:25:46.960563Z","iopub.execute_input":"2023-04-06T13:25:46.961113Z","iopub.status.idle":"2023-04-06T13:25:48.174479Z","shell.execute_reply.started":"2023-04-06T13:25:46.961077Z","shell.execute_reply":"2023-04-06T13:25:48.173663Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Model: \"model_1\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_2 (InputLayer)           [(None, 256, 256, 3  0           []                               \n                                )]                                                                \n                                                                                                  \n patches_1 (Patches)            (None, 256, 768)     0           ['input_2[0][0]']                \n                                                                                                  \n patch_encoder_1 (PatchEncoder)  (None, 256, 64)     65600       ['patches_1[0][0]']              \n                                                                                                  \n layer_normalization_17 (LayerN  (None, 256, 64)     128         ['patch_encoder_1[0][0]']        \n ormalization)                                                                                    \n                                                                                                  \n multi_head_attention_8 (MultiH  (None, 256, 64)     99520       ['layer_normalization_17[0][0]', \n eadAttention)                                                    'layer_normalization_17[0][0]'] \n                                                                                                  \n add_16 (Add)                   (None, 256, 64)      0           ['multi_head_attention_8[0][0]', \n                                                                  'patch_encoder_1[0][0]']        \n                                                                                                  \n layer_normalization_18 (LayerN  (None, 256, 64)     128         ['add_16[0][0]']                 \n ormalization)                                                                                    \n                                                                                                  \n dense_21 (Dense)               (None, 256, 128)     8320        ['layer_normalization_18[0][0]'] \n                                                                                                  \n dropout_19 (Dropout)           (None, 256, 128)     0           ['dense_21[0][0]']               \n                                                                                                  \n dense_22 (Dense)               (None, 256, 64)      8256        ['dropout_19[0][0]']             \n                                                                                                  \n dropout_20 (Dropout)           (None, 256, 64)      0           ['dense_22[0][0]']               \n                                                                                                  \n add_17 (Add)                   (None, 256, 64)      0           ['dropout_20[0][0]',             \n                                                                  'add_16[0][0]']                 \n                                                                                                  \n layer_normalization_19 (LayerN  (None, 256, 64)     128         ['add_17[0][0]']                 \n ormalization)                                                                                    \n                                                                                                  \n multi_head_attention_9 (MultiH  (None, 256, 64)     99520       ['layer_normalization_19[0][0]', \n eadAttention)                                                    'layer_normalization_19[0][0]'] \n                                                                                                  \n add_18 (Add)                   (None, 256, 64)      0           ['multi_head_attention_9[0][0]', \n                                                                  'add_17[0][0]']                 \n                                                                                                  \n layer_normalization_20 (LayerN  (None, 256, 64)     128         ['add_18[0][0]']                 \n ormalization)                                                                                    \n                                                                                                  \n dense_23 (Dense)               (None, 256, 128)     8320        ['layer_normalization_20[0][0]'] \n                                                                                                  \n dropout_21 (Dropout)           (None, 256, 128)     0           ['dense_23[0][0]']               \n                                                                                                  \n dense_24 (Dense)               (None, 256, 64)      8256        ['dropout_21[0][0]']             \n                                                                                                  \n dropout_22 (Dropout)           (None, 256, 64)      0           ['dense_24[0][0]']               \n                                                                                                  \n add_19 (Add)                   (None, 256, 64)      0           ['dropout_22[0][0]',             \n                                                                  'add_18[0][0]']                 \n                                                                                                  \n layer_normalization_21 (LayerN  (None, 256, 64)     128         ['add_19[0][0]']                 \n ormalization)                                                                                    \n                                                                                                  \n multi_head_attention_10 (Multi  (None, 256, 64)     99520       ['layer_normalization_21[0][0]', \n HeadAttention)                                                   'layer_normalization_21[0][0]'] \n                                                                                                  \n add_20 (Add)                   (None, 256, 64)      0           ['multi_head_attention_10[0][0]',\n                                                                  'add_19[0][0]']                 \n                                                                                                  \n layer_normalization_22 (LayerN  (None, 256, 64)     128         ['add_20[0][0]']                 \n ormalization)                                                                                    \n                                                                                                  \n dense_25 (Dense)               (None, 256, 128)     8320        ['layer_normalization_22[0][0]'] \n                                                                                                  \n dropout_23 (Dropout)           (None, 256, 128)     0           ['dense_25[0][0]']               \n                                                                                                  \n dense_26 (Dense)               (None, 256, 64)      8256        ['dropout_23[0][0]']             \n                                                                                                  \n dropout_24 (Dropout)           (None, 256, 64)      0           ['dense_26[0][0]']               \n                                                                                                  \n add_21 (Add)                   (None, 256, 64)      0           ['dropout_24[0][0]',             \n                                                                  'add_20[0][0]']                 \n                                                                                                  \n layer_normalization_23 (LayerN  (None, 256, 64)     128         ['add_21[0][0]']                 \n ormalization)                                                                                    \n                                                                                                  \n multi_head_attention_11 (Multi  (None, 256, 64)     99520       ['layer_normalization_23[0][0]', \n HeadAttention)                                                   'layer_normalization_23[0][0]'] \n                                                                                                  \n add_22 (Add)                   (None, 256, 64)      0           ['multi_head_attention_11[0][0]',\n                                                                  'add_21[0][0]']                 \n                                                                                                  \n layer_normalization_24 (LayerN  (None, 256, 64)     128         ['add_22[0][0]']                 \n ormalization)                                                                                    \n                                                                                                  \n dense_27 (Dense)               (None, 256, 128)     8320        ['layer_normalization_24[0][0]'] \n                                                                                                  \n dropout_25 (Dropout)           (None, 256, 128)     0           ['dense_27[0][0]']               \n                                                                                                  \n dense_28 (Dense)               (None, 256, 64)      8256        ['dropout_25[0][0]']             \n                                                                                                  \n dropout_26 (Dropout)           (None, 256, 64)      0           ['dense_28[0][0]']               \n                                                                                                  \n add_23 (Add)                   (None, 256, 64)      0           ['dropout_26[0][0]',             \n                                                                  'add_22[0][0]']                 \n                                                                                                  \n layer_normalization_25 (LayerN  (None, 256, 64)     128         ['add_23[0][0]']                 \n ormalization)                                                                                    \n                                                                                                  \n multi_head_attention_12 (Multi  (None, 256, 64)     99520       ['layer_normalization_25[0][0]', \n HeadAttention)                                                   'layer_normalization_25[0][0]'] \n                                                                                                  \n add_24 (Add)                   (None, 256, 64)      0           ['multi_head_attention_12[0][0]',\n                                                                  'add_23[0][0]']                 \n                                                                                                  \n layer_normalization_26 (LayerN  (None, 256, 64)     128         ['add_24[0][0]']                 \n ormalization)                                                                                    \n                                                                                                  \n dense_29 (Dense)               (None, 256, 128)     8320        ['layer_normalization_26[0][0]'] \n                                                                                                  \n dropout_27 (Dropout)           (None, 256, 128)     0           ['dense_29[0][0]']               \n                                                                                                  \n dense_30 (Dense)               (None, 256, 64)      8256        ['dropout_27[0][0]']             \n                                                                                                  \n dropout_28 (Dropout)           (None, 256, 64)      0           ['dense_30[0][0]']               \n                                                                                                  \n add_25 (Add)                   (None, 256, 64)      0           ['dropout_28[0][0]',             \n                                                                  'add_24[0][0]']                 \n                                                                                                  \n layer_normalization_27 (LayerN  (None, 256, 64)     128         ['add_25[0][0]']                 \n ormalization)                                                                                    \n                                                                                                  \n multi_head_attention_13 (Multi  (None, 256, 64)     99520       ['layer_normalization_27[0][0]', \n HeadAttention)                                                   'layer_normalization_27[0][0]'] \n                                                                                                  \n add_26 (Add)                   (None, 256, 64)      0           ['multi_head_attention_13[0][0]',\n                                                                  'add_25[0][0]']                 \n                                                                                                  \n layer_normalization_28 (LayerN  (None, 256, 64)     128         ['add_26[0][0]']                 \n ormalization)                                                                                    \n                                                                                                  \n dense_31 (Dense)               (None, 256, 128)     8320        ['layer_normalization_28[0][0]'] \n                                                                                                  \n dropout_29 (Dropout)           (None, 256, 128)     0           ['dense_31[0][0]']               \n                                                                                                  \n dense_32 (Dense)               (None, 256, 64)      8256        ['dropout_29[0][0]']             \n                                                                                                  \n dropout_30 (Dropout)           (None, 256, 64)      0           ['dense_32[0][0]']               \n                                                                                                  \n add_27 (Add)                   (None, 256, 64)      0           ['dropout_30[0][0]',             \n                                                                  'add_26[0][0]']                 \n                                                                                                  \n layer_normalization_29 (LayerN  (None, 256, 64)     128         ['add_27[0][0]']                 \n ormalization)                                                                                    \n                                                                                                  \n multi_head_attention_14 (Multi  (None, 256, 64)     99520       ['layer_normalization_29[0][0]', \n HeadAttention)                                                   'layer_normalization_29[0][0]'] \n                                                                                                  \n add_28 (Add)                   (None, 256, 64)      0           ['multi_head_attention_14[0][0]',\n                                                                  'add_27[0][0]']                 \n                                                                                                  \n layer_normalization_30 (LayerN  (None, 256, 64)     128         ['add_28[0][0]']                 \n ormalization)                                                                                    \n                                                                                                  \n dense_33 (Dense)               (None, 256, 128)     8320        ['layer_normalization_30[0][0]'] \n                                                                                                  \n dropout_31 (Dropout)           (None, 256, 128)     0           ['dense_33[0][0]']               \n                                                                                                  \n dense_34 (Dense)               (None, 256, 64)      8256        ['dropout_31[0][0]']             \n                                                                                                  \n dropout_32 (Dropout)           (None, 256, 64)      0           ['dense_34[0][0]']               \n                                                                                                  \n add_29 (Add)                   (None, 256, 64)      0           ['dropout_32[0][0]',             \n                                                                  'add_28[0][0]']                 \n                                                                                                  \n layer_normalization_31 (LayerN  (None, 256, 64)     128         ['add_29[0][0]']                 \n ormalization)                                                                                    \n                                                                                                  \n multi_head_attention_15 (Multi  (None, 256, 64)     99520       ['layer_normalization_31[0][0]', \n HeadAttention)                                                   'layer_normalization_31[0][0]'] \n                                                                                                  \n add_30 (Add)                   (None, 256, 64)      0           ['multi_head_attention_15[0][0]',\n                                                                  'add_29[0][0]']                 \n                                                                                                  \n layer_normalization_32 (LayerN  (None, 256, 64)     128         ['add_30[0][0]']                 \n ormalization)                                                                                    \n                                                                                                  \n dense_35 (Dense)               (None, 256, 128)     8320        ['layer_normalization_32[0][0]'] \n                                                                                                  \n dropout_33 (Dropout)           (None, 256, 128)     0           ['dense_35[0][0]']               \n                                                                                                  \n dense_36 (Dense)               (None, 256, 64)      8256        ['dropout_33[0][0]']             \n                                                                                                  \n dropout_34 (Dropout)           (None, 256, 64)      0           ['dense_36[0][0]']               \n                                                                                                  \n add_31 (Add)                   (None, 256, 64)      0           ['dropout_34[0][0]',             \n                                                                  'add_30[0][0]']                 \n                                                                                                  \n layer_normalization_33 (LayerN  (None, 256, 64)     128         ['add_31[0][0]']                 \n ormalization)                                                                                    \n                                                                                                  \n flatten_1 (Flatten)            (None, 16384)        0           ['layer_normalization_33[0][0]'] \n                                                                                                  \n dropout_35 (Dropout)           (None, 16384)        0           ['flatten_1[0][0]']              \n                                                                                                  \n dense_37 (Dense)               (None, 512)          8389120     ['dropout_35[0][0]']             \n                                                                                                  \n dropout_36 (Dropout)           (None, 512)          0           ['dense_37[0][0]']               \n                                                                                                  \n dense_38 (Dense)               (None, 256)          131328      ['dropout_36[0][0]']             \n                                                                                                  \n dropout_37 (Dropout)           (None, 256)          0           ['dense_38[0][0]']               \n                                                                                                  \n dense_39 (Dense)               (None, 6)            1542        ['dropout_37[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 9,518,534\nTrainable params: 9,518,534\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = tfa.optimizers.AdamW(\n        learning_rate=0.001, weight_decay=0.0001\n    )\n\nmodel.compile(\n        optimizer=optimizer,\n        loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n        metrics='Accuracy'\n        \n    )","metadata":{"execution":{"iopub.status.busy":"2023-04-06T13:25:48.175583Z","iopub.execute_input":"2023-04-06T13:25:48.176157Z","iopub.status.idle":"2023-04-06T13:25:48.213129Z","shell.execute_reply.started":"2023-04-06T13:25:48.176116Z","shell.execute_reply":"2023-04-06T13:25:48.212343Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"#model.fit(train_generator,validation_data =validation_generator,epochs = 3)\nSTEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\nSTEP_SIZE_VALID=validation_generator.n//validation_generator.batch_size\nmodel.fit_generator(generator=train_generator,\n                    steps_per_epoch=STEP_SIZE_TRAIN,\n                    validation_data=validation_generator,\n                    validation_steps=STEP_SIZE_VALID,\n                    epochs=10\n)","metadata":{"execution":{"iopub.status.busy":"2023-04-06T13:25:48.214137Z","iopub.execute_input":"2023-04-06T13:25:48.214514Z","iopub.status.idle":"2023-04-06T13:48:06.156348Z","shell.execute_reply.started":"2023-04-06T13:25:48.214477Z","shell.execute_reply":"2023-04-06T13:48:06.153227Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n87/87 [==============================] - 278s 3s/step - loss: 2.2197 - Accuracy: 0.2132 - val_loss: 1.6110 - val_Accuracy: 0.2995\nEpoch 2/10\n87/87 [==============================] - 263s 3s/step - loss: 1.6930 - Accuracy: 0.2558 - val_loss: 1.6250 - val_Accuracy: 0.3110\nEpoch 3/10\n87/87 [==============================] - 266s 3s/step - loss: 1.6496 - Accuracy: 0.2792 - val_loss: 1.5724 - val_Accuracy: 0.3389\nEpoch 4/10\n87/87 [==============================] - 264s 3s/step - loss: 1.6291 - Accuracy: 0.2809 - val_loss: 1.5087 - val_Accuracy: 0.3426\nEpoch 5/10\n43/87 [=============>................] - ETA: 1:33 - loss: 1.5384 - Accuracy: 0.3319","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/487220822.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTEP_SIZE_VALID\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2616\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2617\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2618\u001b[0;31m             \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2619\u001b[0m         )\n\u001b[1;32m   2620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1648\u001b[0m                         ):\n\u001b[1;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m    134\u001b[0m     return concrete_function._call_flat(\n\u001b[0;32m--> 135\u001b[0;31m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    381\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 53\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"model.save('test-vision-encoder')","metadata":{"execution":{"iopub.status.busy":"2023-04-05T17:12:34.211231Z","iopub.execute_input":"2023-04-05T17:12:34.212317Z","iopub.status.idle":"2023-04-05T17:12:50.768001Z","shell.execute_reply.started":"2023-04-05T17:12:34.212264Z","shell.execute_reply":"2023-04-05T17:12:50.766856Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nimport shutil\nshutil.make_archive('test-vision-encoder-output', 'zip', '/kaggle/working/test-vision-encoder')\n","metadata":{"execution":{"iopub.status.busy":"2023-04-05T17:24:07.273507Z","iopub.execute_input":"2023-04-05T17:24:07.274132Z","iopub.status.idle":"2023-04-05T17:24:31.552586Z","shell.execute_reply.started":"2023-04-05T17:24:07.274072Z","shell.execute_reply":"2023-04-05T17:24:31.551563Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/test-vision-encoder-output.zip'"},"metadata":{}}]},{"cell_type":"code","source":"FileLink('/kaggle/working/test-vision-encoder-output.zip')","metadata":{"execution":{"iopub.status.busy":"2023-04-05T17:32:08.386296Z","iopub.execute_input":"2023-04-05T17:32:08.387392Z","iopub.status.idle":"2023-04-05T17:32:08.395151Z","shell.execute_reply.started":"2023-04-05T17:32:08.387348Z","shell.execute_reply":"2023-04-05T17:32:08.394061Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/test-vision-encoder-output.zip","text/html":"<a href='/kaggle/working/test-vision-encoder-output.zip' target='_blank'>/kaggle/working/test-vision-encoder-output.zip</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"model = keras.models.load_model('/kaggle/input/vision-encoder-model')\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-04-05T18:01:37.986200Z","iopub.execute_input":"2023-04-05T18:01:37.986635Z","iopub.status.idle":"2023-04-05T18:01:44.908649Z","shell.execute_reply.started":"2023-04-05T18:01:37.986599Z","shell.execute_reply":"2023-04-05T18:01:44.907803Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_1 (InputLayer)           [(None, 256, 256, 3  0           []                               \n                                )]                                                                \n                                                                                                  \n patches (Patches)              (None, 256, 768)     0           ['input_1[0][0]']                \n                                                                                                  \n patch_encoder (PatchEncoder)   (None, 256, 64)      65600       ['patches[0][0]']                \n                                                                                                  \n layer_normalization (LayerNorm  (None, 256, 64)     128         ['patch_encoder[0][0]']          \n alization)                                                                                       \n                                                                                                  \n multi_head_attention (MultiHea  (None, 256, 64)     99520       ['layer_normalization[0][0]',    \n dAttention)                                                      'layer_normalization[0][0]']    \n                                                                                                  \n add (Add)                      (None, 256, 64)      0           ['multi_head_attention[0][0]',   \n                                                                  'patch_encoder[0][0]']          \n                                                                                                  \n layer_normalization_1 (LayerNo  (None, 256, 64)     128         ['add[0][0]']                    \n rmalization)                                                                                     \n                                                                                                  \n dense_1 (Dense)                (None, 256, 128)     8320        ['layer_normalization_1[0][0]']  \n                                                                                                  \n dropout (Dropout)              (None, 256, 128)     0           ['dense_1[0][0]']                \n                                                                                                  \n dense_2 (Dense)                (None, 256, 64)      8256        ['dropout[0][0]']                \n                                                                                                  \n dropout_1 (Dropout)            (None, 256, 64)      0           ['dense_2[0][0]']                \n                                                                                                  \n add_1 (Add)                    (None, 256, 64)      0           ['dropout_1[0][0]',              \n                                                                  'add[0][0]']                    \n                                                                                                  \n layer_normalization_2 (LayerNo  (None, 256, 64)     128         ['add_1[0][0]']                  \n rmalization)                                                                                     \n                                                                                                  \n multi_head_attention_1 (MultiH  (None, 256, 64)     99520       ['layer_normalization_2[0][0]',  \n eadAttention)                                                    'layer_normalization_2[0][0]']  \n                                                                                                  \n add_2 (Add)                    (None, 256, 64)      0           ['multi_head_attention_1[0][0]', \n                                                                  'add_1[0][0]']                  \n                                                                                                  \n layer_normalization_3 (LayerNo  (None, 256, 64)     128         ['add_2[0][0]']                  \n rmalization)                                                                                     \n                                                                                                  \n dense_3 (Dense)                (None, 256, 128)     8320        ['layer_normalization_3[0][0]']  \n                                                                                                  \n dropout_2 (Dropout)            (None, 256, 128)     0           ['dense_3[0][0]']                \n                                                                                                  \n dense_4 (Dense)                (None, 256, 64)      8256        ['dropout_2[0][0]']              \n                                                                                                  \n dropout_3 (Dropout)            (None, 256, 64)      0           ['dense_4[0][0]']                \n                                                                                                  \n add_3 (Add)                    (None, 256, 64)      0           ['dropout_3[0][0]',              \n                                                                  'add_2[0][0]']                  \n                                                                                                  \n layer_normalization_4 (LayerNo  (None, 256, 64)     128         ['add_3[0][0]']                  \n rmalization)                                                                                     \n                                                                                                  \n multi_head_attention_2 (MultiH  (None, 256, 64)     99520       ['layer_normalization_4[0][0]',  \n eadAttention)                                                    'layer_normalization_4[0][0]']  \n                                                                                                  \n add_4 (Add)                    (None, 256, 64)      0           ['multi_head_attention_2[0][0]', \n                                                                  'add_3[0][0]']                  \n                                                                                                  \n layer_normalization_5 (LayerNo  (None, 256, 64)     128         ['add_4[0][0]']                  \n rmalization)                                                                                     \n                                                                                                  \n dense_5 (Dense)                (None, 256, 128)     8320        ['layer_normalization_5[0][0]']  \n                                                                                                  \n dropout_4 (Dropout)            (None, 256, 128)     0           ['dense_5[0][0]']                \n                                                                                                  \n dense_6 (Dense)                (None, 256, 64)      8256        ['dropout_4[0][0]']              \n                                                                                                  \n dropout_5 (Dropout)            (None, 256, 64)      0           ['dense_6[0][0]']                \n                                                                                                  \n add_5 (Add)                    (None, 256, 64)      0           ['dropout_5[0][0]',              \n                                                                  'add_4[0][0]']                  \n                                                                                                  \n layer_normalization_6 (LayerNo  (None, 256, 64)     128         ['add_5[0][0]']                  \n rmalization)                                                                                     \n                                                                                                  \n multi_head_attention_3 (MultiH  (None, 256, 64)     99520       ['layer_normalization_6[0][0]',  \n eadAttention)                                                    'layer_normalization_6[0][0]']  \n                                                                                                  \n add_6 (Add)                    (None, 256, 64)      0           ['multi_head_attention_3[0][0]', \n                                                                  'add_5[0][0]']                  \n                                                                                                  \n layer_normalization_7 (LayerNo  (None, 256, 64)     128         ['add_6[0][0]']                  \n rmalization)                                                                                     \n                                                                                                  \n dense_7 (Dense)                (None, 256, 128)     8320        ['layer_normalization_7[0][0]']  \n                                                                                                  \n dropout_6 (Dropout)            (None, 256, 128)     0           ['dense_7[0][0]']                \n                                                                                                  \n dense_8 (Dense)                (None, 256, 64)      8256        ['dropout_6[0][0]']              \n                                                                                                  \n dropout_7 (Dropout)            (None, 256, 64)      0           ['dense_8[0][0]']                \n                                                                                                  \n add_7 (Add)                    (None, 256, 64)      0           ['dropout_7[0][0]',              \n                                                                  'add_6[0][0]']                  \n                                                                                                  \n layer_normalization_8 (LayerNo  (None, 256, 64)     128         ['add_7[0][0]']                  \n rmalization)                                                                                     \n                                                                                                  \n multi_head_attention_4 (MultiH  (None, 256, 64)     99520       ['layer_normalization_8[0][0]',  \n eadAttention)                                                    'layer_normalization_8[0][0]']  \n                                                                                                  \n add_8 (Add)                    (None, 256, 64)      0           ['multi_head_attention_4[0][0]', \n                                                                  'add_7[0][0]']                  \n                                                                                                  \n layer_normalization_9 (LayerNo  (None, 256, 64)     128         ['add_8[0][0]']                  \n rmalization)                                                                                     \n                                                                                                  \n dense_9 (Dense)                (None, 256, 128)     8320        ['layer_normalization_9[0][0]']  \n                                                                                                  \n dropout_8 (Dropout)            (None, 256, 128)     0           ['dense_9[0][0]']                \n                                                                                                  \n dense_10 (Dense)               (None, 256, 64)      8256        ['dropout_8[0][0]']              \n                                                                                                  \n dropout_9 (Dropout)            (None, 256, 64)      0           ['dense_10[0][0]']               \n                                                                                                  \n add_9 (Add)                    (None, 256, 64)      0           ['dropout_9[0][0]',              \n                                                                  'add_8[0][0]']                  \n                                                                                                  \n layer_normalization_10 (LayerN  (None, 256, 64)     128         ['add_9[0][0]']                  \n ormalization)                                                                                    \n                                                                                                  \n multi_head_attention_5 (MultiH  (None, 256, 64)     99520       ['layer_normalization_10[0][0]', \n eadAttention)                                                    'layer_normalization_10[0][0]'] \n                                                                                                  \n add_10 (Add)                   (None, 256, 64)      0           ['multi_head_attention_5[0][0]', \n                                                                  'add_9[0][0]']                  \n                                                                                                  \n layer_normalization_11 (LayerN  (None, 256, 64)     128         ['add_10[0][0]']                 \n ormalization)                                                                                    \n                                                                                                  \n dense_11 (Dense)               (None, 256, 128)     8320        ['layer_normalization_11[0][0]'] \n                                                                                                  \n dropout_10 (Dropout)           (None, 256, 128)     0           ['dense_11[0][0]']               \n                                                                                                  \n dense_12 (Dense)               (None, 256, 64)      8256        ['dropout_10[0][0]']             \n                                                                                                  \n dropout_11 (Dropout)           (None, 256, 64)      0           ['dense_12[0][0]']               \n                                                                                                  \n add_11 (Add)                   (None, 256, 64)      0           ['dropout_11[0][0]',             \n                                                                  'add_10[0][0]']                 \n                                                                                                  \n layer_normalization_12 (LayerN  (None, 256, 64)     128         ['add_11[0][0]']                 \n ormalization)                                                                                    \n                                                                                                  \n multi_head_attention_6 (MultiH  (None, 256, 64)     99520       ['layer_normalization_12[0][0]', \n eadAttention)                                                    'layer_normalization_12[0][0]'] \n                                                                                                  \n add_12 (Add)                   (None, 256, 64)      0           ['multi_head_attention_6[0][0]', \n                                                                  'add_11[0][0]']                 \n                                                                                                  \n layer_normalization_13 (LayerN  (None, 256, 64)     128         ['add_12[0][0]']                 \n ormalization)                                                                                    \n                                                                                                  \n dense_13 (Dense)               (None, 256, 128)     8320        ['layer_normalization_13[0][0]'] \n                                                                                                  \n dropout_12 (Dropout)           (None, 256, 128)     0           ['dense_13[0][0]']               \n                                                                                                  \n dense_14 (Dense)               (None, 256, 64)      8256        ['dropout_12[0][0]']             \n                                                                                                  \n dropout_13 (Dropout)           (None, 256, 64)      0           ['dense_14[0][0]']               \n                                                                                                  \n add_13 (Add)                   (None, 256, 64)      0           ['dropout_13[0][0]',             \n                                                                  'add_12[0][0]']                 \n                                                                                                  \n layer_normalization_14 (LayerN  (None, 256, 64)     128         ['add_13[0][0]']                 \n ormalization)                                                                                    \n                                                                                                  \n multi_head_attention_7 (MultiH  (None, 256, 64)     99520       ['layer_normalization_14[0][0]', \n eadAttention)                                                    'layer_normalization_14[0][0]'] \n                                                                                                  \n add_14 (Add)                   (None, 256, 64)      0           ['multi_head_attention_7[0][0]', \n                                                                  'add_13[0][0]']                 \n                                                                                                  \n layer_normalization_15 (LayerN  (None, 256, 64)     128         ['add_14[0][0]']                 \n ormalization)                                                                                    \n                                                                                                  \n dense_15 (Dense)               (None, 256, 128)     8320        ['layer_normalization_15[0][0]'] \n                                                                                                  \n dropout_14 (Dropout)           (None, 256, 128)     0           ['dense_15[0][0]']               \n                                                                                                  \n dense_16 (Dense)               (None, 256, 64)      8256        ['dropout_14[0][0]']             \n                                                                                                  \n dropout_15 (Dropout)           (None, 256, 64)      0           ['dense_16[0][0]']               \n                                                                                                  \n add_15 (Add)                   (None, 256, 64)      0           ['dropout_15[0][0]',             \n                                                                  'add_14[0][0]']                 \n                                                                                                  \n layer_normalization_16 (LayerN  (None, 256, 64)     128         ['add_15[0][0]']                 \n ormalization)                                                                                    \n                                                                                                  \n flatten (Flatten)              (None, 16384)        0           ['layer_normalization_16[0][0]'] \n                                                                                                  \n dropout_16 (Dropout)           (None, 16384)        0           ['flatten[0][0]']                \n                                                                                                  \n dense_17 (Dense)               (None, 2048)         33556480    ['dropout_16[0][0]']             \n                                                                                                  \n dropout_17 (Dropout)           (None, 2048)         0           ['dense_17[0][0]']               \n                                                                                                  \n dense_18 (Dense)               (None, 1024)         2098176     ['dropout_17[0][0]']             \n                                                                                                  \n dropout_18 (Dropout)           (None, 1024)         0           ['dense_18[0][0]']               \n                                                                                                  \n dense_19 (Dense)               (None, 6)            6150        ['dropout_18[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 36,657,350\nTrainable params: 36,657,350\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","output_type":"stream"}]}]}